{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTで文書をベクトル化して、そのベクトルを使って検索エンジンを開発する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae8e336dec34680ab6191b0f2e43245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051d567718e74dbdad1f57ad715623cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SHONOSUKE/Addtional_Trained_BERT_For_Legal_Domain_v1 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tohoku-nlp/bert-base-japanese-whole-word-masking\")\n",
    "model = BertModel.from_pretrained(\"SHONOSUKE/Addtional_Trained_BERT_For_Legal_Domain_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"私は法律について学んでいます。他にもたくさんの学問を追求していて、その中で法律に興味を持ちました。さらに、法律の中でも特に民法に興味があります。\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0742e-01,  8.4026e-01,  3.9895e-01, -6.0953e-01, -1.1792e+00,\n",
      "        -4.6073e-01,  3.2532e-01,  3.6992e-01,  1.0273e+00,  4.7997e-01,\n",
      "        -2.1563e-01, -1.5731e-01,  7.6189e-01, -1.6167e-01, -9.3144e-01,\n",
      "         4.9808e-01, -7.1130e-01,  2.3955e-01,  2.0671e-01, -2.5446e-01,\n",
      "         1.7583e-01, -1.7249e-01, -8.2031e-01,  9.0697e-01,  5.2804e-01,\n",
      "         1.6546e-01,  1.0069e-01, -1.2478e+00, -9.8018e-01,  6.9510e-01,\n",
      "         4.2980e-01, -4.4547e-02, -3.3894e-01,  1.4479e-03,  1.8889e-02,\n",
      "         4.8537e-01, -6.4466e-01,  1.3762e+00,  1.1029e-02, -1.5044e-01,\n",
      "        -3.4801e-01, -5.7987e-01, -4.4971e-01,  9.6070e-01, -6.9530e-02,\n",
      "         2.2931e-02,  2.4871e-01, -4.8105e-01, -1.7094e-01, -5.4150e-02,\n",
      "         1.4301e-01,  6.1162e-01,  4.5249e-02, -2.8746e-02, -5.0524e-01,\n",
      "         2.5274e-01, -2.6187e-01, -5.1871e-01, -4.4538e-01, -1.5823e-01,\n",
      "        -4.2285e-01, -6.3670e-01, -4.8037e-02, -3.2277e-01,  9.9222e-02,\n",
      "        -5.0372e-01,  5.0011e-01, -1.7863e-01, -4.3756e-01,  9.9057e-02,\n",
      "        -9.4597e-02,  9.1448e-02,  2.0146e-01, -7.4930e-02,  4.3685e-01,\n",
      "        -3.9511e-01,  7.2927e-01, -9.0155e-02,  7.2229e-02,  9.3347e-01,\n",
      "         5.7437e-01, -5.1891e-02, -6.0102e-01, -1.3954e-01,  8.0337e-01,\n",
      "         8.0328e-01, -1.2761e-01, -1.2488e-01,  1.8119e-01,  2.2137e+00,\n",
      "        -3.4634e-01,  4.8173e-01,  9.1427e-01,  1.6148e-01, -4.8359e-01,\n",
      "        -1.4350e+00,  5.4561e-01,  1.6894e+00,  8.3078e-01,  3.0169e-01,\n",
      "         1.2905e-01, -6.3227e-01, -8.0322e-02,  8.7946e-02, -4.0731e-01,\n",
      "         1.3567e-01, -4.7124e-01, -1.3192e-01, -2.5734e-01,  7.0143e-01,\n",
      "         3.6367e-01,  1.5593e-01, -3.2286e-02, -3.6267e-01, -1.8897e-01,\n",
      "        -8.4972e-01,  2.9976e-01, -5.6132e-01, -1.5183e-01, -6.2123e-02,\n",
      "         8.0430e-01, -1.1833e+00,  4.6608e-01, -4.7415e-02, -9.6987e-03,\n",
      "         6.8407e-01, -5.3572e-02,  6.1142e-01, -4.7700e-04, -3.1581e-01,\n",
      "        -8.0011e-01, -8.3848e-02,  4.0816e-01, -7.8565e-01,  6.2713e-02,\n",
      "        -6.6321e-01, -9.3979e-02,  1.1375e-01, -4.0188e-03,  1.9772e-02,\n",
      "         9.2807e-01,  1.6548e-01, -2.0202e-01, -4.8738e-01,  3.6219e-01,\n",
      "        -4.9156e-01,  5.5078e-01, -3.3591e-01,  2.4413e-02,  5.8156e-01,\n",
      "        -8.3684e-01, -3.2408e-01,  3.6346e-01,  3.5990e-01, -1.1035e-02,\n",
      "        -1.1142e-01,  3.7673e-01,  1.0407e+00,  2.8101e-01, -3.5915e-01,\n",
      "         7.0458e-01, -4.6652e-01, -8.9927e-01, -6.9136e-01,  2.0229e-01,\n",
      "         1.3317e+00, -3.7770e-01,  7.6809e-01, -8.1859e-01, -5.6909e-01,\n",
      "        -1.5287e-01, -7.3192e-01,  2.2071e-01,  4.5843e-01, -2.1940e-01,\n",
      "        -4.2058e-01, -2.7075e-01, -2.6120e-02,  9.6860e-01,  4.2382e-01,\n",
      "        -7.3094e-01,  5.2578e-01,  1.1219e+00, -6.5060e-01, -6.9004e-01,\n",
      "         4.0199e-01,  2.5598e-01, -7.4417e-01, -4.6561e-02, -1.3693e+00,\n",
      "         6.3830e-02, -2.1748e-01,  1.0687e+00,  2.6445e-01,  9.3874e-02,\n",
      "        -6.0315e-01, -3.4419e-01,  1.2681e-01,  5.7169e-01,  8.4717e-02,\n",
      "        -1.9034e-01,  3.6497e-01,  8.0359e-01,  5.3020e-01,  9.1536e-01,\n",
      "         6.0080e-01, -4.0972e-01, -8.9776e-01,  2.6069e-01,  8.1693e-01,\n",
      "         8.1159e-01,  1.2054e-01,  2.0759e-01,  1.1587e+00, -8.4712e-02,\n",
      "        -2.5172e-02,  9.0466e-01, -4.8213e-01, -5.9598e-01, -6.9078e-01,\n",
      "        -7.0510e-01, -1.1554e-01,  6.3366e-01,  2.2770e-01,  5.0794e-01,\n",
      "        -6.0353e-01, -6.3758e-01,  7.7232e-01, -1.7143e-01,  5.0663e-01,\n",
      "        -2.9727e-01, -4.5108e-01, -6.1625e-01, -2.6108e+00,  3.1428e-01,\n",
      "         2.6376e-01, -2.3736e-01,  5.4088e-02,  7.6833e-01, -5.5520e-01,\n",
      "         6.5795e-02, -3.0430e-01, -5.7851e-01,  1.7901e-01,  8.3109e-01,\n",
      "         1.4944e+00, -1.1880e+00,  6.8619e-01, -1.7073e-01, -6.7678e-01,\n",
      "         6.0241e-01, -4.0898e-01, -2.8530e-01,  2.1100e-01, -1.0856e-01,\n",
      "        -9.0680e-01, -5.9540e-01,  1.5033e+00,  1.0064e+00,  2.9284e-01,\n",
      "        -6.5222e-01,  5.8734e-01, -1.2444e-01, -4.5658e-01, -1.9556e+00,\n",
      "         4.2415e-01, -6.7443e-01,  1.5567e-01,  2.7729e-01, -5.1740e-01,\n",
      "         1.1060e+00, -7.6995e-01, -7.8837e-01, -2.5960e-01, -4.2226e-02,\n",
      "        -2.1206e-01,  1.2881e+00, -2.9574e-01, -4.9548e-02,  8.8795e-02,\n",
      "        -8.2740e-01,  1.6232e-01, -5.2450e-01, -4.4773e-01, -2.0019e-01,\n",
      "         1.1065e-01,  5.9307e-01,  5.5769e-01,  1.1762e-01, -9.1030e-01,\n",
      "        -2.1696e-01,  1.8810e-01, -7.2068e-01,  1.1247e-01,  6.0381e-01,\n",
      "         7.4961e-01, -1.1902e-01,  1.1341e-01,  1.2664e-01, -3.3670e-01,\n",
      "        -3.0929e-01, -1.5043e+00, -4.5520e-01, -1.0394e-01, -1.4597e-01,\n",
      "         1.0149e+00, -3.2234e-01,  2.2623e-01,  3.6963e-01, -2.9651e-01,\n",
      "        -8.6958e-01,  1.8884e-01, -2.7361e-01,  7.4566e-01,  4.3147e-01,\n",
      "        -8.8959e-01, -2.0107e-01, -3.0745e-01, -5.3504e-01,  7.6727e-01,\n",
      "        -8.3754e-02, -7.8133e-01,  9.4456e-02, -1.7035e-01,  2.6143e-02,\n",
      "        -9.3990e-02,  6.0034e-01, -8.5795e-01, -1.3930e+00,  1.6403e-01,\n",
      "        -6.1765e-02,  2.4355e-01,  5.2587e-01,  1.3869e-01, -4.9251e-01,\n",
      "         3.3574e-01, -1.2507e+00,  3.7183e-01,  6.3393e-01,  5.4944e-01,\n",
      "        -3.0101e-01, -1.0905e+00,  1.8397e-01, -1.6265e-02,  6.7713e-01,\n",
      "        -3.7095e-03, -2.9893e-02,  1.1176e+00,  4.1037e-01, -2.4345e-01,\n",
      "         1.0400e-01,  1.7298e-01, -1.1824e+00, -8.2110e-01,  3.1094e-01,\n",
      "        -7.1302e-01, -1.3067e-01,  1.0602e+00, -1.6006e-02,  3.4013e-01,\n",
      "         1.2792e+00,  2.5876e-01, -2.6133e-01, -6.9542e-01, -1.0560e-01,\n",
      "        -1.3734e-01,  4.2328e-02,  1.4793e+00, -9.1549e-01, -8.5595e-01,\n",
      "         1.0860e-01,  3.4898e-01,  4.0873e-01,  6.9139e-02,  5.3246e-02,\n",
      "         2.6386e-01,  3.6296e-01,  1.2662e+00,  3.3384e-02, -8.7300e-01,\n",
      "         5.9466e-01,  5.0460e-01,  1.8528e-01,  7.1314e-01,  5.5489e-01,\n",
      "        -1.7691e-01, -7.7242e-01, -6.2322e-01,  6.0547e-01, -6.6921e-02,\n",
      "        -4.4918e-01,  2.4880e-01, -4.1017e-01,  3.5016e-01, -9.7890e-01,\n",
      "         3.7126e-01, -8.5399e-02,  3.5454e-01, -1.0884e-01,  2.7084e-01,\n",
      "        -3.5425e-01, -6.7933e-01,  1.3555e-02,  8.2176e-01,  1.1853e+00,\n",
      "         4.6582e-01, -2.8691e-01,  7.4756e-01,  4.1171e-01, -2.9001e-01,\n",
      "         3.1521e-01, -3.9573e-01, -7.2879e-01, -2.1304e-01,  2.6628e-01,\n",
      "         4.8934e-01, -5.8692e-01,  3.7167e-01, -7.5594e-01, -2.5437e-01,\n",
      "         3.0119e-01, -4.9466e-01, -3.8038e-01, -2.1308e-01,  7.5200e-01,\n",
      "         4.0584e-01,  1.0543e+00,  1.0327e-01,  1.3128e+00, -7.3629e-01,\n",
      "         2.1198e-01,  7.9593e-01,  9.2118e-02,  2.8334e-01,  6.3941e-02,\n",
      "         8.5601e-01, -1.7178e-01,  5.3825e-01, -9.0141e-01, -1.3516e-01,\n",
      "         2.3395e-01,  9.1591e-01,  1.0370e-01, -4.4840e-01, -1.2060e-01,\n",
      "         3.6910e-01, -5.7449e-01,  6.3186e-01, -7.9752e-01,  7.8264e-02,\n",
      "        -4.7939e-01,  6.2996e-02,  3.4657e-02,  7.1897e-03,  5.4488e-01,\n",
      "         2.6166e-01,  7.2277e-01,  1.1431e-01, -2.2440e-01,  3.2572e-01,\n",
      "         4.6184e-01,  4.0059e-01,  3.4061e-01,  1.1314e+00, -7.7860e-01,\n",
      "        -4.6692e-01, -2.3161e-01, -4.6151e-01, -1.9271e-01, -2.8888e-01,\n",
      "         1.4276e-01,  5.9054e-01,  1.1476e+00, -5.6534e-01, -5.3105e-02,\n",
      "        -3.3372e-01, -8.8886e-01,  1.0870e-01, -7.8238e-02, -1.5472e-01,\n",
      "        -1.4273e-01, -5.0860e-01, -4.5171e-01, -4.1635e-01,  1.2195e-01,\n",
      "         2.7549e-02, -4.2283e-01,  3.5706e-01,  3.9614e-01, -5.0716e-01,\n",
      "        -5.0236e-01,  3.5473e-01, -9.4883e-01,  5.7237e-01, -1.7718e-01,\n",
      "        -3.4313e-02,  1.4443e-01,  5.0825e-01,  7.4902e-01,  2.0162e-01,\n",
      "         8.8433e-01,  2.8699e-01, -9.7483e-01, -6.2437e-01,  7.7135e-03,\n",
      "        -7.0463e-01,  3.8152e-01,  1.4937e-01, -2.8284e-02, -4.4973e-01,\n",
      "        -2.3889e-01, -5.9867e-02, -1.2501e+00,  2.8967e-01,  4.8934e-01,\n",
      "        -4.1681e-01,  1.7675e+00,  1.1353e-01, -5.2366e-01, -4.7218e-01,\n",
      "         3.0371e-01,  3.2791e-02,  9.6963e-01, -5.3475e-01,  1.0699e+00,\n",
      "        -7.4800e-02,  8.8293e-01, -2.1688e-01,  6.4443e-02, -1.1238e-01,\n",
      "         4.8273e-01, -8.0424e-01, -2.2133e-02, -2.9862e-02, -1.4502e-01,\n",
      "         1.1266e-02, -3.9822e-01,  3.4694e-01, -7.3785e-01, -2.2108e-01,\n",
      "         2.3499e-01,  3.1404e-01,  7.0140e-01, -6.9739e-02,  3.0342e-01,\n",
      "        -3.4292e-03, -2.6347e-01,  4.1928e-03,  1.4451e-01, -8.3660e-01,\n",
      "         6.7164e-01,  6.5017e-01, -1.5842e-02,  7.1269e-01, -7.5238e-01,\n",
      "         1.4583e-01, -1.7942e-02, -4.6001e-01,  7.3208e-01,  6.8231e-01,\n",
      "         8.6122e-01, -5.4826e-01, -4.0665e-01,  1.8872e-01, -7.6878e-01,\n",
      "        -9.7999e-01,  6.7943e-01,  5.9185e-01,  9.2991e-02,  5.0356e-01,\n",
      "        -5.8739e-01,  4.7812e-01, -6.3691e-01,  1.4493e-01, -1.7228e-01,\n",
      "         5.3447e-01,  1.6749e-01, -2.6698e-01,  1.4437e-01,  3.4460e-01,\n",
      "        -7.9952e-01,  5.6627e-01, -5.8617e-01,  4.3380e-01,  3.8376e-02,\n",
      "        -5.6529e-02, -3.1981e-01, -1.9439e-02, -6.7656e-01, -3.2419e-01,\n",
      "        -8.1037e-01,  1.4746e+00,  3.1824e-01, -4.1716e-01, -6.0403e-01,\n",
      "        -2.6679e-02, -4.0445e-02, -4.8374e-01,  1.9941e-01,  2.8987e-02,\n",
      "         6.7889e-01,  7.9256e-01,  3.4830e-01, -3.7946e-01, -5.4682e-02,\n",
      "         6.4972e-01,  5.6119e-02,  1.1258e-01,  8.4091e-02,  1.0184e-02,\n",
      "         4.1696e-01, -8.5891e-01,  6.2415e-01, -3.1765e-02,  1.2124e+00,\n",
      "        -1.1118e+00, -1.9688e-01,  3.5481e+00, -3.3426e-02,  3.7337e-01,\n",
      "         1.0361e+00, -2.9451e-03, -9.1704e-01, -1.0016e+01,  7.7233e-01,\n",
      "         6.0356e-01,  2.4647e-01, -2.2936e-01,  2.1330e-01, -1.9077e-01,\n",
      "         3.5497e-01,  7.9185e-01,  2.5023e-01,  1.4541e-01, -6.3923e-01,\n",
      "        -9.8010e-02,  9.1720e-01, -7.1675e-01,  1.4578e-01, -1.9364e-01,\n",
      "        -4.7968e-01,  1.1071e+00, -5.0380e-01, -4.2217e-01,  5.8370e-01,\n",
      "         7.5261e-01, -7.3005e-01,  4.4762e-01, -1.1230e+00,  2.0593e-01,\n",
      "        -4.1663e-01, -3.2517e-01,  1.0065e+00, -1.3332e-02, -6.5109e-01,\n",
      "        -4.0455e-01,  3.3936e-01, -6.5028e-01,  2.6315e-02,  1.9376e-01,\n",
      "         4.0950e-01, -5.8351e-01, -1.3988e+00, -7.5873e-01,  2.7754e-01,\n",
      "         5.7908e-01, -5.5397e-01,  6.7235e-01,  2.1224e-02,  7.2105e-01,\n",
      "         2.5102e-01, -4.3839e-01,  5.0042e-01,  5.1368e-01,  6.7437e-01,\n",
      "         6.5616e-01,  2.1699e-03,  1.0508e+00, -4.0323e-01, -1.3320e-01,\n",
      "         8.5512e-02, -3.7359e-01, -5.2342e-01,  5.7756e-01,  2.0772e-01,\n",
      "         4.1102e-03,  2.2953e-01, -4.4363e-01, -1.4879e+00,  3.2092e-01,\n",
      "         1.9345e-01,  2.4612e-01, -1.8977e-01, -4.5888e-01,  3.1420e-01,\n",
      "        -1.2069e+00, -2.8891e-01,  2.4169e-01, -4.4929e-01,  7.9264e-01,\n",
      "        -2.3404e-01, -9.1080e-01,  7.3797e-02,  1.0030e-01,  3.4202e-01,\n",
      "        -3.5946e-01, -5.3243e-01,  7.1174e-02, -9.1533e-02, -2.1201e-01,\n",
      "         3.3568e-01,  2.9048e-01, -3.3506e-01, -9.7536e-01,  5.8143e-01,\n",
      "        -1.5144e-01,  7.5764e-01,  7.9323e-01,  6.1130e-01, -9.5721e-01,\n",
      "         6.5382e-01,  5.2791e-01, -2.0877e-01, -1.6781e-01,  1.4049e-01,\n",
      "         6.5742e-01, -5.1092e-01, -1.5100e-01, -2.8877e-01,  8.8713e-01,\n",
      "        -8.0960e-01,  1.6861e-01,  9.5560e-01,  1.9698e-01,  1.4773e-01,\n",
      "        -4.8975e-02,  8.9379e-01, -8.2159e-02,  1.1552e+00,  4.0762e-01,\n",
      "        -2.5321e-01,  7.0321e-01, -5.3732e-01, -6.1798e-01,  7.3796e-02,\n",
      "         8.7260e-01, -4.0674e-01, -4.1321e-01,  5.5811e-01, -4.0127e-01,\n",
      "         3.8126e-01, -2.4293e-01, -1.0976e-01,  2.4106e-01,  7.9524e-01,\n",
      "        -5.6390e-01, -5.4575e-02,  5.7951e-01, -5.2248e-01, -4.8654e-03,\n",
      "         8.3263e-01,  3.5063e-01, -6.8946e-01, -4.4147e-01, -1.5006e-01,\n",
      "        -9.5692e-01,  2.7108e-01,  1.4843e-02])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# [CLS]トークンの出力を文書のベクトルとして使用\n",
    "doc_vector = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実際に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        return data\n",
    "\n",
    "clean_dataset = load_file('clean_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [value[\"gist\"] for value in clean_dataset.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SHONOSUKE/Addtional_Trained_BERT_For_Legal_Domain_v1 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tohoku-nlp/bert-base-japanese-whole-word-masking\")\n",
    "model = BertModel.from_pretrained(\"SHONOSUKE/Addtional_Trained_BERT_For_Legal_Domain_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "num_dimention = model.config.hidden_size\n",
    "print(num_dimention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30716 is processing\n",
      "2 / 30716 is processing\n",
      "3 / 30716 is processing\n",
      "4 / 30716 is processing\n",
      "5 / 30716 is processing\n",
      "6 / 30716 is processing\n",
      "7 / 30716 is processing\n",
      "8 / 30716 is processing\n",
      "9 / 30716 is processing\n",
      "10 / 30716 is processing\n",
      "11 / 30716 is processing\n",
      "12 / 30716 is processing\n",
      "13 / 30716 is processing\n",
      "14 / 30716 is processing\n",
      "15 / 30716 is processing\n",
      "16 / 30716 is processing\n",
      "17 / 30716 is processing\n",
      "18 / 30716 is processing\n",
      "19 / 30716 is processing\n",
      "20 / 30716 is processing\n",
      "21 / 30716 is processing\n",
      "22 / 30716 is processing\n",
      "23 / 30716 is processing\n",
      "24 / 30716 is processing\n",
      "25 / 30716 is processing\n",
      "26 / 30716 is processing\n",
      "27 / 30716 is processing\n",
      "28 / 30716 is processing\n",
      "29 / 30716 is processing\n",
      "30 / 30716 is processing\n",
      "31 / 30716 is processing\n",
      "32 / 30716 is processing\n",
      "33 / 30716 is processing\n",
      "34 / 30716 is processing\n",
      "35 / 30716 is processing\n",
      "36 / 30716 is processing\n",
      "37 / 30716 is processing\n",
      "38 / 30716 is processing\n",
      "39 / 30716 is processing\n",
      "40 / 30716 is processing\n",
      "41 / 30716 is processing\n",
      "42 / 30716 is processing\n",
      "43 / 30716 is processing\n",
      "44 / 30716 is processing\n",
      "45 / 30716 is processing\n",
      "46 / 30716 is processing\n",
      "47 / 30716 is processing\n",
      "48 / 30716 is processing\n",
      "49 / 30716 is processing\n",
      "50 / 30716 is processing\n",
      "51 / 30716 is processing\n",
      "52 / 30716 is processing\n",
      "53 / 30716 is processing\n",
      "54 / 30716 is processing\n",
      "55 / 30716 is processing\n",
      "56 / 30716 is processing\n",
      "57 / 30716 is processing\n",
      "58 / 30716 is processing\n",
      "59 / 30716 is processing\n",
      "60 / 30716 is processing\n",
      "61 / 30716 is processing\n",
      "62 / 30716 is processing\n",
      "63 / 30716 is processing\n",
      "64 / 30716 is processing\n",
      "65 / 30716 is processing\n",
      "66 / 30716 is processing\n",
      "67 / 30716 is processing\n",
      "68 / 30716 is processing\n",
      "69 / 30716 is processing\n",
      "70 / 30716 is processing\n",
      "71 / 30716 is processing\n",
      "72 / 30716 is processing\n",
      "73 / 30716 is processing\n",
      "74 / 30716 is processing\n",
      "75 / 30716 is processing\n",
      "76 / 30716 is processing\n",
      "77 / 30716 is processing\n",
      "78 / 30716 is processing\n",
      "79 / 30716 is processing\n",
      "80 / 30716 is processing\n",
      "81 / 30716 is processing\n",
      "82 / 30716 is processing\n",
      "83 / 30716 is processing\n",
      "84 / 30716 is processing\n",
      "85 / 30716 is processing\n",
      "86 / 30716 is processing\n",
      "87 / 30716 is processing\n",
      "88 / 30716 is processing\n",
      "89 / 30716 is processing\n",
      "90 / 30716 is processing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m doc_vector \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     10\u001b[0m corpus_vectors\u001b[38;5;241m.\u001b[39mappend(doc_vector)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/transformers/pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythonnenv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus_vectors = []\n",
    "num = 1\n",
    "for text in corpus:\n",
    "    print(f\"{num} / {len(corpus)} is processing\")\n",
    "    num += 1\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    doc_vector = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    corpus_vectors.append(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from voyager import Index, Space\n",
    "\n",
    "index = Index(Space.Cosine, num_dimensions=768)\n",
    "vectors = np.stack(corpus_vectors, axis=0)\n",
    "_ = index.add_items(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save(\"BERT.voy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Index.load(\"BERT.voy\")\n",
    "\n",
    "query_documents = [\n",
    "    \"小学校における殺人事件\"\n",
    "]\n",
    "\n",
    "query_vectors = []\n",
    "for text in query_documents:\n",
    "    # 入力をトークナイズし、GPUに移動\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    doc_vector = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "doc_vector_array = doc_vector.cpu().detach().numpy()\n",
    "neighbors, distances = index.query(doc_vector_array, 5)\n",
    "\n",
    "neighbors = neighbors.tolist()\n",
    "\n",
    "for i in range(len(neighbors)):\n",
    "    print(clean_dataset[list(clean_dataset.keys())[neighbors[i]]][\"case_name\"])\n",
    "    print(clean_dataset[list(clean_dataset.keys())[neighbors[i]]][\"gist\"])\n",
    "    print(clean_dataset[list(clean_dataset.keys())[neighbors[i]]][\"detail_page_link\"])\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
